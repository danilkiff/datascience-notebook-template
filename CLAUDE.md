<!-- SPDX-License-Identifier: Unlicense -->

# CLAUDE.md

## Project overview

Docker-first DS/ML template: JupyterLab + MLflow + PostgreSQL + MinIO.
No Python source code in the repo — only infrastructure files.
User code lives in `workspace/` (gitignored contents, tracked structure).

## Architecture

Five core Docker Compose services with health-checked startup order:

```text
s3 (MinIO) ─healthy─▶ init_s3 (bucket setup) ─completed─┐
postgres ─healthy────────────────────────────────────────┤
                                                         ▼
                                                       mlflow ─healthy─▶ jupyter
```

Optional profiles: `monitoring` (Prometheus + Grafana),
`orchestration` (Prefect), `serving` (MLflow model serving).

Two networks: `backend` (postgres, mlflow, minio) and
`frontend` (jupyter, mlflow, minio).
MinIO is in both networks — MLflow accesses it via backend,
Jupyter uses it via frontend for DVC.
MLflow artifacts are proxied through MLflow (`--serve-artifacts`);
DVC uses MinIO directly via the frontend network.
GPU support is a separate overlay: `docker-compose.gpu.yaml`.

## Key files

- `docker-compose.yaml` — service orchestration, single source of truth
- `docker-compose.gpu.yaml` — NVIDIA GPU overlay (shm\_size + devices)
- `Dockerfile.jupyter` — base: scipy-notebook, adds fastai + mlflow
- `Dockerfile.mlflow` — base: python:3.12-slim, adds mlflow + psycopg2
- `Dockerfile.prefect` — base: python:3.12-slim, adds prefect (optional)
- `Dockerfile.serving` — base: python:3.12-slim, adds mlflow serving
  (optional)
- `requirements/*.in` — top-level deps (jupyter, mlflow, prefect, serving)
- `requirements/*.txt` — locked deps (generated by `uv pip compile`)
- `copier.yml` — Copier template configuration
- `scripts/` — `init-env.sh`, `init-postgres.sql`, `serve-model.sh`
- `.env.example` — all configurable variables with defaults
- `Makefile` — common commands (`make help` to list)

## Common commands

```bash
make init           # generate .env with random passwords
make up             # start stack (CPU)
make up-gpu         # start stack with NVIDIA GPU
make up-monitoring  # start with Prometheus + Grafana
make up-orchestration # start with Prefect
make up-serving     # start with model serving
make down           # stop
make clean          # stop + remove volumes
make logs           # tail all service logs
make build          # rebuild images without cache
make lock           # regenerate locked deps from *.in
make train          # run training script in Jupyter
make test           # run pytest in Jupyter
make dvc-push       # push DVC data to MinIO
make dvc-pull       # pull DVC data from MinIO
```

## Conventions

- All Docker images and pip packages must be version-pinned
- Environment variables go through `.env`, never hardcoded in compose
- `env_file:` is not used — explicit `environment:` map only
- Internal services (postgres, minio API) must not expose ports to host
- Only Jupyter (8888), MLflow UI, MinIO Console are exposed
- Commit messages: imperative mood, short first line (see git log)
- Source files carry `SPDX-License-Identifier: Unlicense`
  (Dockerfiles, compose, Makefile, CI workflow, markdown docs —
  not dotfiles, .env, or requirements)
- Markdown headings use sentence case (capitalize first word only)

## CI

Path-filtered jobs via `dorny/paths-filter` — each job runs only
when relevant files change:

| Job          | Triggers on                                         |
| ------------ | --------------------------------------------------- |
| hadolint     | `Dockerfile.*`, `requirements/**`                   |
| compose-lint | `docker-compose*.yaml`, Dockerfiles, `.env.example` |
| markdownlint | `**/*.md`, `.markdownlint.yaml`                     |
| actionlint   | `.github/workflows/**`                              |
| smoke-test   | same as compose-lint                                |
| gitleaks     | always (all files)                                  |

Hadolint matrix covers: `Dockerfile.jupyter`, `Dockerfile.mlflow`,
`Dockerfile.prefect`, `Dockerfile.serving`.

Local: pre-commit hooks — nbstripout + ruff.

## Adding Python dependencies

1. Add pinned package to `requirements/jupyter.in` or
   `requirements/mlflow.in`
2. Run `make lock` to regenerate the `.txt` lock files
3. Run `make build`

## Data versioning with DVC

DVC is pre-configured with MinIO as remote storage (`s3://dvc`).
Inside the Jupyter container:

1. `dvc add data/raw/my_dataset.csv`
2. `git add data/raw/my_dataset.csv.dvc data/raw/.gitignore`
3. `make dvc-push` (or `dvc push`)

## Adding a new service

1. Add service definition to `docker-compose.yaml`
2. Assign to `backend` and/or `frontend` network
3. Add healthcheck so downstream services can `depends_on`
4. Pin image version, never use `latest`
5. If the service needs secrets, pass via `environment:` map
6. Update `.env.example` with new variables and defaults

## Modifying CI

1. Edit `.github/workflows/ci.yml`
2. If adding a new job, add a path filter in the `changes` job
3. Set `needs: changes` and appropriate `if:` condition
4. Run `actionlint` locally before pushing

## Do not

- Put secrets in tracked files (`.env` is gitignored)
- Add `env_file:` back to docker-compose services
- Expose postgres or minio API ports to host
- Hardcode `shm_size` — use `SHM_SIZE` env var (default 2g)
- Commit notebook outputs — nbstripout pre-commit hook handles this
- Use Title Case in markdown headings — sentence case only
- Use `fetch-depth: 0` unless the job needs full git history
